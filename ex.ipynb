{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# virtualenv 하는 법 [ 리눅스 ]\n",
    "# pip3 install virtualenv\n",
    "# virtualenv [name of virtualenv]\n",
    "# virtualenv [name of virtualenv] --python=3.6\n",
    "# source [name of virtualenv]/bin/activate\n",
    "#deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mps = 맥북의 gpu 가속 모드 \n",
    "import torch\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda = nvidia GPU 가속 모드\n",
    "# developer.nvidia.com/cuda-toolkit-archive => GPU와 파이썬 버전 확인 필요\n",
    "# cuDNN도 설치 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 기본연산 ]\n",
    "# (scalar)스칼라 => 하나의 값\n",
    "# (tensor)벡터 => 두개 이상의 수치로 표현한 값\n",
    "    # torch.add/sub/mul/div/dot => 사칙 연산 함수\n",
    "# (tensor)행렬 => 두개 이상의 벡터값으로 구성된 값 / \"연산 속도를 빠르게 진행할 수 있는 선형 대수의 기본 단위\"\n",
    "    # matmul : 행렬곱\n",
    "# (tensor)텐서 => 2차원 이상의 배열  / 이거 곱 계산은 두가지 ( 그냥곱 / 행렬곱 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    print(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH_SIZE, INPUT_SIZE, device = DEVICE, dtype = torch.float, requires_grad = False)\n",
    "\n",
    "y = torch.randn(BATCH_SIZE, OUTPUT_SIZE, device = DEVICE, dtype = torch.float, requires_grad = False)\n",
    "\n",
    "w1 = torch.randn(INPUT_SIZE, HIDDEN_SIZE, device = DEVICE, dtype = torch.float, requires_grad = True)\n",
    "\n",
    "w2 = torch.randn(HIDDEN_SIZE, OUTPUT_SIZE, device = DEVICE, dtype = torch.float, requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  100 \t Loss:  534.28759765625\n",
      "Iteration :  200 \t Loss:  3.391860008239746\n",
      "Iteration :  300 \t Loss:  0.040327977389097214\n",
      "Iteration :  400 \t Loss:  0.0008995939861051738\n",
      "Iteration :  500 \t Loss:  0.00010313434177078307\n"
     ]
    }
   ],
   "source": [
    "# 데이터와 가중치 x, y, w1, w2 가 정의되어 있다는 가정 하에\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    # Forward Pass : 예측값 계산\n",
    "    # mm : 행렬곱, clamp(min = 0)은 ReLU 활성화 함수 \n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2) # 이건 예측값\n",
    "    \n",
    "    # Loss 계산 (MSE와 유사한 형태))\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t %100 ==0:\n",
    "        print(\"Iteration : \", t ,\"\\t\", \"Loss: \", loss.item())\n",
    "        \n",
    "    # Backward Pass : Autograd가 미분(기울기) 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. 가중치 업데이트( 기록 안남기려고 no_grad 사용)\n",
    "    with torch.no_grad( ):\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 기울기 초기화\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd vs gridsearchCV ( : 파라미터 관련이라)\n",
    "    - 파라미터 vs 하이퍼파라미터\n",
    "        => 파라미터 : 가중치 , 편향\n",
    "        => 하이퍼파라미터 ㅣ 학습률, 배치 크기, 층 수 \n",
    "    - gridsearch : 하이퍼 파라미터, 사용자가 미리 정해준 것 대입 후 가장 성과가 좋은 것 적용\n",
    "    - Autograd : 파라미터, 어떤 가중치를 고려해야 정답에 가까워질지 손실실함수를 미분하여 기울기 구해서 제안 -> optimizer 통해서 \n",
    "\n",
    "## Batch_Size : 파라미터를 업데이트 할 때, 계산되는 데이터의 개수\n",
    "## Hidden_size : 결과에 한번 더 계산되는 파라미터 수 \n",
    "\n",
    "## gradient : 기울기와 방향 \n",
    "## Relu 함수 : x에 w1층 곱하고 렐르함수 적용, 이후 w2 다시 곱함 ( 이 라인에서는 행렬곱 )\n",
    "## 이후 오차.pow(2).sum()로 제곱합을 얻어냄\n",
    "## 그 다음, bakward()로 gradient 계산\n",
    "\n",
    "## 후 해당 시점의 gradient 값 고정\n",
    "\n",
    "## w1.grad.lr - w1\n",
    "## w2.grad.lr - w2 이러한 가중치 변화는 덮어쓰기화 됨\n",
    "\n",
    "## 결국, 한번에 루프에서 일어나는 것 \n",
    "    # 가중치는 계속해서 덮어쓰기 됨\n",
    "    # gradient는 기존 수치가 반영되기 때문에 초기화해서 새롭게 구해진 가중치로 구한 새로운 gradient를 구해서 다시 가중치 조정이 이뤄지는 것임\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
